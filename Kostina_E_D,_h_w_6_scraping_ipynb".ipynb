{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekaterina-Kostina/study/blob/main/Kostina_E_D%2C_h_w_6_scraping_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1**\n",
        "\n",
        "Написать функцию для скрейпинга веб-страниц"
      ],
      "metadata": {
        "id": "vi_saniub3hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 -q\n",
        "!pip install requests -q\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "aWlZpSILeLJN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Допишите функцию для скрейпинга\n",
        "'''\n",
        "\n",
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200: ### если запрос успешный (возвращается значение 200) ###\n",
        "      soup = BeautifulSoup(response.text, 'html.parser') ### парсим контент или текст с помощью BeautifulSoup html.parser ###\n",
        "      paragraphs = soup.find_all(tag, class_) # ищем теги и классы на основании аргументов функции; по умолчанию ищем только тег\n",
        "      text = [] ### создаем переменную text строкового типа, куда мы запишем все найденные совпадения ###\n",
        "      for i in paragraphs: ### начинаем перебор элементов переменной paragraphs ###\n",
        "        text.append(i.get_text(strip=True)) # извлекаем текстовые данные\n",
        "      return text\n",
        "  else: ### иначе ###\n",
        "      print(f\"{response.status_code} Error\") ### выводим статус-код и сообщение об ошибке ###\n",
        "      return []"
      ],
      "metadata": {
        "id": "AI-6i4LWeL82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Проверяем функцию для скрейпинга: пример 1\n",
        "'''\n",
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200: ### если запрос успешный (возвращается значение 200) ###\n",
        "      soup = BeautifulSoup(response.text, 'html.parser') ### парсим контент или текст с помощью BeautifulSoup html.parser ###\n",
        "      paragraphs = soup.find_all(tag, class_) # ищем теги и классы на основании аргументов функции; по умолчанию ищем только тег\n",
        "      text = [] ### создаем переменную text строкового типа, куда мы запишем все найденные совпадения ###\n",
        "      for p in paragraphs: ### начинаем перебор элементов переменной paragraphs ###\n",
        "        text.append(p.get_text(strip=True)) # извлекаем текстовые данные\n",
        "      return text\n",
        "  else: ### иначе ###\n",
        "      print(f\"{response.status_code} Error\") ### выводим статус-код и сообщение об ошибке ###\n",
        "      return []\n",
        "\n",
        "scrape_text_from_url('https://en.wikipedia.org/wiki/Chomsky_hierarchy', 'p')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrKlqhx5fcGB",
        "outputId": "d9e4a24a-3ba2-4d96-c323-96dce10d6795"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"TheChomsky hierarchyin the fields offormal language theory,computer science, andlinguistics, is acontainment hierarchyof classes offormal grammars. A formal grammar describes how to form strings from a language's vocabulary (or alphabet) that are valid according to the language's syntax. The linguistNoam Chomskytheorized that four different classes of formal grammars existed that could generate increasingly complex languages. Each class can also completely generate the language of all inferior classes (set inclusive).\",\n",
              " 'The general idea of a hierarchy of grammars was first described by Noam Chomsky in \"Three models for the description of language\" during the formalization oftransformational-generative grammar(TGG).[1]Marcel-Paul Schützenbergeralso played a role in the development of the theory offormal languages; the paper \"The algebraic theory of context free languages\"[2]describes the modern hierarchy, including context-free grammars.[3]',\n",
              " 'Independently, alongside linguists, mathematicians were developing models of computation (viaautomata). Parsing a sentence in a language is similar to computation, and the grammars described by Chomsky proved to both resemble and be equivalent in computational power to various machine models.[4]',\n",
              " \"The following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have. The classes are defined by the constraints on the productions rules.\",\n",
              " 'Note that the set of grammars corresponding torecursive languagesis not a member of this hierarchy; these would be properly between Type-0 and Type-1.',\n",
              " 'Every regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.[7]',\n",
              " 'Type-3 grammars generate theregular languages. Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal, in which case the grammar isright regular. Alternatively, all the rules can have their right-hand sides consist of a single terminal, possiblyprecededby a single nonterminal (left regular). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The ruleS→ε{\\\\displaystyle S\\\\rightarrow \\\\varepsilon }is also allowed here ifS{\\\\displaystyle S}does not appear on the right side of any rule. These languages are exactly all languages that can be decided by afinite-state automaton. Additionally, this family of formal languages can be obtained byregular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.',\n",
              " 'For example, the regular languageL={an|n>0}{\\\\displaystyle L=\\\\{a^{n}|n>0\\\\}}is generated by the Type-3 grammarG=({S},{a,b},P,S){\\\\displaystyle G=(\\\\{S\\\\},\\\\{a,b\\\\},P,S)}with the productionsP{\\\\displaystyle P}being the following.',\n",
              " 'Type-2 grammars generate thecontext-free languages. These are defined by rules of the formA→α{\\\\displaystyle A\\\\rightarrow \\\\alpha }withA{\\\\displaystyle A}being a nonterminal andα{\\\\displaystyle \\\\alpha }being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministicpushdown automaton. Context-free languages—or rather its subset ofdeterministic context-free languages—are the theoretical basis for the phrase structure of mostprogramming languages, though their syntax also includes context-sensitivename resolutiondue to declarations andscope. Often a subset of grammars is used to make parsing easier, such as by anLL parser.',\n",
              " 'For example, the context-free languageL={anbn|n>0}{\\\\displaystyle L=\\\\{a^{n}b^{n}|n>0\\\\}}is generated by the Type-2 grammarG=({S},{a,b},P,S){\\\\displaystyle G=(\\\\{S\\\\},\\\\{a,b\\\\},P,S)}with the productionsP{\\\\displaystyle P}being the following.',\n",
              " 'The language is context-free but not regular (by thepumping lemma for regular languages).',\n",
              " 'Type-1 grammars generatecontext-sensitive languages. These grammars have rules of the formαAβ→αγβ{\\\\displaystyle \\\\alpha A\\\\beta \\\\rightarrow \\\\alpha \\\\gamma \\\\beta }withA{\\\\displaystyle A}a nonterminal andα{\\\\displaystyle \\\\alpha },β{\\\\displaystyle \\\\beta }andγ{\\\\displaystyle \\\\gamma }strings of terminals and/or nonterminals. The stringsα{\\\\displaystyle \\\\alpha }andβ{\\\\displaystyle \\\\beta }may be empty, butγ{\\\\displaystyle \\\\gamma }must be nonempty.  The ruleS→ϵ{\\\\displaystyle S\\\\rightarrow \\\\epsilon }is allowed ifS{\\\\displaystyle S}does not appear on the right side of any rule.  The languages described by these grammars are exactly all languages that can be recognized by alinear bounded automaton(a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)',\n",
              " 'For example, the context-sensitive languageL={anbncn|n>0}{\\\\displaystyle L=\\\\{a^{n}b^{n}c^{n}|n>0\\\\}}is generated by the Type-1 grammarG=({S,A,B,C,W,Z},{a,b},P,S){\\\\displaystyle G=(\\\\{S,A,B,C,W,Z\\\\},\\\\{a,b\\\\},P,S)}with the productionsP{\\\\displaystyle P}being the following.',\n",
              " 'The language is context-sensitive but not context-free (by thepumping lemma for context-free languages).\\nA proof that this grammar generatesL={anbncn|n>0}{\\\\displaystyle L=\\\\{a^{n}b^{n}c^{n}|n>0\\\\}}is sketched in the article onContext-sensitive grammars.',\n",
              " 'Type-0 grammars include all formal grammars. There are no constraints on the productions rules. They generate exactly all languages that can be recognized by aTuring machine, thus any language that is possible to be generated can be generated by a Type-0 grammar.[8]These languages are also known as therecursively enumerableorTuring-recognizablelanguages.[8]Note that this is different from therecursive languages, which can bedecidedby analways-halting Turing machine.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Проверяем функцию для скрейпинга: пример 2\n",
        "'''\n",
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200: ### если запрос успешный (возвращается значение 200) ###\n",
        "      soup = BeautifulSoup(response.text, 'html.parser') ### парсим контент или текст с помощью BeautifulSoup html.parser ###\n",
        "      paragraphs = soup.find_all(tag, class_) # ищем теги и классы на основании аргументов функции; по умолчанию ищем только тег\n",
        "      text = [] ### создаем переменную text строкового типа, куда мы запишем все найденные совпадения ###\n",
        "      for p in paragraphs: ### начинаем перебор элементов переменной paragraphs ###\n",
        "        text.append(p.get_text(strip=True)) # извлекаем текстовые данные\n",
        "      return text\n",
        "  else: ### иначе ###\n",
        "      print(f\"{response.status_code} Error\") ### выводим статус-код и сообщение об ошибке ###\n",
        "      return []\n",
        "scrape_text_from_url('https://www.rottentomatoes.com/m/civil_war_2024/reviews', 'p', 'review-text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz6rp1Gsfi2K",
        "outputId": "6409f8f1-d5a1-4b51-ba55-9e0ffd018b0b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Civil War is not a film about why extreme division happens; it’s about the personal impact of division at its worst.',\n",
              " 'A staggering and frequently brilliant film looking at a hopefully just fictionalized version of America destroying itself from within.',\n",
              " 'Garland seems poised to be make political points, to show how our current political dysfunction could lead to something far worse… but he never follows through.',\n",
              " 'This violent tale of anaesthetised reporters is just a war yarn with a twist.',\n",
              " 'This easy digital fakeness is there to get the audience used to it...',\n",
              " 'Rather than glorifying war, the film is artful and meditative, making us feel the pointlessness of this violence, how unglamorous and exhausting and unheroic it is.',\n",
              " 'It’s not always subtle, and in some cases it’s even frustrating with some of its narrative decisions, but nothing can deny its uncanny ability to leave you horrified by how war, civil or otherwise, can be truly monstrous from any humanist angle.',\n",
              " 'The film isn’t very deep, but it does paint the grimmest picture possible of a divided nation with bodies bodies bodies everywhere.',\n",
              " 'With a career best performance from Kirsten Dunst and a pulsating sound design, Civil War is a thrilling body of work that should be talked about in journalism circles.',\n",
              " \"Garland's decision to forego any narrative antecedent makes the predictable ending that much more of a bummer. Civil War is a more muscular film than Men. Like that movie, however, it mistakes broad brush strokes for insight.\",\n",
              " 'Audiences expecting a meticulously-crafted backstory for how this war started and who the players are will be sorely disappointed.',\n",
              " \"This is another war-is-awful movie, tracing the experiences of a team of photojournalists who throw themselves into the firing line. And even without much of a point, it's rivetingly well-made.\",\n",
              " 'Garland uses violence as a means to a much larger end that should leave moviegoers disturbed and, at times, sickened; that should leave us challenged to think rather than feeling any kind of welcome catharsis. The final shot ... is stunning in its irony.',\n",
              " 'The film seems caught between wanting to be exciting and devastating, as it threatens to turn its captivating story into another generic shoot-them-up war film, just on a grander scale.',\n",
              " 'Civil War is not a polemic, but it is a mirror to the USA’s political crisis.',\n",
              " 'If beauty is the sole standard for their work, then the film’s view of war photography is as an aesthetic death cult.',\n",
              " 'Garland appears to make movies less for audiences than as think pieces, and, given the esteem in which his films are held, the ploy has paid off handsomely.',\n",
              " 'An urgent film about a barely-distant future.',\n",
              " 'There’s a cautionary message then in the madness: stop or perish; the choice is entirely ours.',\n",
              " 'Civil War is a reflection of what you bring to it. That puts the impetus on the viewer to meet the film on its level. Some won’t like what they find. The nature of that discussion is what will ultimately make this film immortal.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2**\n",
        "Написать функцию для сентимент-анализа"
      ],
      "metadata": {
        "id": "4yAvz7QEb8yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Загружаем списки слов с положительным и отрицательным значением\n",
        "'''\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/neg.txt\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/pos.txt\n",
        "\n",
        "with open('neg.txt') as f:\n",
        "  neg = f.read()\n",
        "\n",
        "with open('pos.txt') as f:\n",
        "  pos = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVqGhbkssw0m",
        "outputId": "a35f4266-7531-4806-a829-ce6917f51fa7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 20:35:42--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/neg.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44759 (44K) [text/plain]\n",
            "Saving to: ‘neg.txt’\n",
            "\n",
            "\rneg.txt               0%[                    ]       0  --.-KB/s               \rneg.txt             100%[===================>]  43.71K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-12-13 20:35:42 (11.7 MB/s) - ‘neg.txt’ saved [44759/44759]\n",
            "\n",
            "--2024-12-13 20:35:43--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/pos.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19093 (19K) [text/plain]\n",
            "Saving to: ‘pos.txt’\n",
            "\n",
            "pos.txt             100%[===================>]  18.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-13 20:35:43 (40.1 MB/s) - ‘pos.txt’ saved [19093/19093]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Создайте списки слов, разделив строку по знаку \\n\n",
        "'''\n",
        "\n",
        "pos_list, neg_list = pos.split(), neg.split() ### ваш код здесь ###\n",
        "\n",
        "pos_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vquXeXlDvG_Z",
        "outputId": "9684b740-03bd-4b09-b238-9c8788ecbbfd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a+',\n",
              " 'abound',\n",
              " 'abounds',\n",
              " 'abundance',\n",
              " 'abundant',\n",
              " 'accessable',\n",
              " 'accessible',\n",
              " 'acclaim',\n",
              " 'acclaimed',\n",
              " 'acclamation']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Допишите функцию для анализа тональности\n",
        "Функция принимает на вход 3 аргумента:\n",
        "  - переменную text с текстов для анализа\n",
        "  - 2 списка со словами для анализа тональности: positive_words и negative_words\n",
        "'''\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt\n",
        "\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split() ### Ваш код здесь: приведите текст к нижнему регистру и поделите его на слова ###\n",
        "    ### По желанию: удалить стоп-слова и знаки препинания; использовать NLP-библиотеки для токенизации ###\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = []\n",
        "    for token in split_text:\n",
        "      if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "        clean_text.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "    words = clean_text ### Важно: запишите результат в переменную words ###\n",
        "\n",
        "    # Считаем скор тональности: количество пересечений между списками words и положительными / отрицательными словами\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    return sentiment\n",
        "     ### Ваш код здесь: если положительный скор больше отрицательного, возвращаем строчку 'positive' ###\n",
        "    ### Ваш код здесь: если отрицательный скор больше положительного, возвращаем строчку 'negative' ###\n",
        "    ### Иначе возвращаем строчку 'neutral' ###"
      ],
      "metadata": {
        "id": "oZ7AmEM1vlL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt -q"
      ],
      "metadata": {
        "id": "vLrYhBHv1eJI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Проверяем функцию для анализа тональности: пример 1\n",
        "'''\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split() ### Ваш код здесь: приведите текст к нижнему регистру и поделите его на слова ###\n",
        "    ### По желанию: удалить стоп-слова и знаки препинания; использовать NLP-библиотеки для токенизации ###\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = []\n",
        "    for token in split_text:\n",
        "      if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "        clean_text.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "    words = clean_text ### Важно: запишите результат в переменную words ###\n",
        "\n",
        "    # Считаем скор тональности: количество пересечений между списками words и положительными / отрицательными словами\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)\n",
        "\n",
        "analyze_sentiment('This sample text is awesome!', pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y31yIOJ6xXS_",
        "outputId": "cb2d1e84-e054-4af6-8790-df22a1a3b777"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Проверяем функцию для анализа тональности: пример 2\n",
        "'''\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split() ### Ваш код здесь: приведите текст к нижнему регистру и поделите его на слова ###\n",
        "    ### По желанию: удалить стоп-слова и знаки препинания; использовать NLP-библиотеки для токенизации ###\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = []\n",
        "    for token in split_text:\n",
        "      if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "        clean_text.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "    words = clean_text ### Важно: запишите результат в переменную words ###\n",
        "\n",
        "    # Считаем скор тональности: количество пересечений между списками words и положительными / отрицательными словами\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)\n",
        "\n",
        "analyze_sentiment('I hate this sample text.', pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r1ubjl49x1qX",
        "outputId": "cb919295-7091-4c7c-b5e4-4f1988fd29cf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Проверяем функцию для анализа тональности: пример 3\n",
        "'''\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split() ### Ваш код здесь: приведите текст к нижнему регистру и поделите его на слова ###\n",
        "    ### По желанию: удалить стоп-слова и знаки препинания; использовать NLP-библиотеки для токенизации ###\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = []\n",
        "    for token in split_text:\n",
        "      if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "        clean_text.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "    words = clean_text ### Важно: запишите результат в переменную words ###\n",
        "\n",
        "    # Считаем скор тональности: количество пересечений между списками words и положительными / отрицательными словами\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)\n",
        "analyze_sentiment('We love and hate this sample text at the same time!', pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lh7-EgSYx7vb",
        "outputId": "25436d72-68fb-4bb8-a41c-68821b3c4477"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 3**\n",
        "\n",
        "Написать функцию для вывода сентимент-скоров"
      ],
      "metadata": {
        "id": "pQe6yWNicClB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Допишите функцию для вывода скоров по заданному URL\n",
        "'''\n",
        "\n",
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_) ### ваш код здесь: примените функцию scrape_text_from_url ###\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = [] ### ваш код здесь: создаем пустой список, куда запишем скоры ###\n",
        "  for sentence in sentences: ### ваш код здесь: начинаем перебор по предложениям ###\n",
        "    score = analyze_sentiment(sentence, positive_words, negative_words) ### ваш код здесь: получаем скор для данного предложения с помощью функции analyze_sentiment ###\n",
        "    sentiments.append(score) ### ваш код здесь: добавляем скор в созданный список ###\n",
        "  return(sentiments) ### ваш код здесь: возвращаем список скоров ###"
      ],
      "metadata": {
        "id": "2x1O80L70uDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Тест 1\n",
        "'''\n",
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_) ### ваш код здесь: примените функцию scrape_text_from_url ###\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = [] ### ваш код здесь: создаем пустой список, куда запишем скоры ###\n",
        "  for sentence in sentences: ### ваш код здесь: начинаем перебор по предложениям ###\n",
        "    score = analyze_sentiment(sentence, positive_words, negative_words) ### ваш код здесь: получаем скор для данного предложения с помощью функции analyze_sentiment ###\n",
        "    sentiments.append(score) ### ваш код здесь: добавляем скор в созданный список ###\n",
        "  return(sentiments) ### ваш код здесь: возвращаем список скоров ###\n",
        "analyze(url='https://en.wikipedia.org/wiki/Chomsky_hierarchy', tag='p', positive_words=pos, negative_words=neg)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc2UL_222Mle",
        "outputId": "5fd7b9e9-81c8-4e68-af2d-8e53a7596a25"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Neutral',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Positive',\n",
              " 'Positive',\n",
              " 'Neutral',\n",
              " 'Neutral',\n",
              " 'Negative',\n",
              " 'Negative']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Тест 2\n",
        "'''\n",
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_) ### ваш код здесь: примените функцию scrape_text_from_url ###\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = [] ### ваш код здесь: создаем пустой список, куда запишем скоры ###\n",
        "  for sentence in sentences: ### ваш код здесь: начинаем перебор по предложениям ###\n",
        "    score = analyze_sentiment(sentence, positive_words, negative_words) ### ваш код здесь: получаем скор для данного предложения с помощью функции analyze_sentiment ###\n",
        "    sentiments.append(score) ### ваш код здесь: добавляем скор в созданный список ###\n",
        "  return(sentiments) ### ваш код здесь: возвращаем список скоров ###\n",
        "analyze(url='https://www.rottentomatoes.com/m/civil_war_2024/reviews', tag='p', class_='review-text', positive_words=pos, negative_words=neg)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9XzHV953Aux",
        "outputId": "8630cdf0-d786-4840-c113-89b6a9819200"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Negative',\n",
              " 'Neutral',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Neutral',\n",
              " 'Neutral',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Neutral']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 4**\n",
        "\n",
        "Визуализировать результаты анализа"
      ],
      "metadata": {
        "id": "-IkiyB5ldbt9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqEXVu-VZcUG",
        "outputId": "5c161039-c34b-4254-8b8e-26d09d264d25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Neutral': 116, 'Negative': 109, 'Positive': 64})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "'''\n",
        "Создаем частотный словарь с помощью библиотеки Counter.\n",
        "Можно выбрать другие веб-страницы для анализа!\n",
        "'''\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "sample1 = Counter(analyze(url='https://en.wikipedia.org/wiki/Capybara', tag='p', positive_words=pos, negative_words=neg))\n",
        "sample2 = Counter(analyze(url='https://en.wikipedia.org/wiki/Red_Dead_Redemption', tag='p', positive_words=pos, negative_words=neg))\n",
        "\n",
        "sample1\n",
        "sample2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Создаем визуализацию: попробуйте использование новые данные (данные других url)\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df1 = pd.DataFrame.from_dict(sample1, orient='index', columns=['Capybaras'])\n",
        "df2 = pd.DataFrame.from_dict(sample2, orient='index', columns=['Red dead Redemption'])\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "\n",
        "ax[0] = df1.plot.bar(ax=ax[0])\n",
        "ax[1] = df2.plot.bar(ax=ax[1], color='orange')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "1BIF0uT84bKN",
        "outputId": "0685ca89-f7e5-479f-bb65-460dfb9b65c6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAHNCAYAAAC+QxloAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAI0lEQVR4nO3de1yUdf7//+egnBEQU0AFxTRPqSWmUm6p0eIhP7Ly3bIstSzTQFO2MvtobaaitnkmbVvDbLNzWlppZh4qDynkqRTRKC0F3QxQWQ7C9fvDH/NpQnEQuGYYHvfbbW4357quueY1F9f77XPe12EshmEYAgAAMImbowsAAAB1C+EDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBU9R1dwB+VlpbqxIkTatCggSwWi6PLAeokwzB09uxZNW3aVG5uteM7Cn0H4FiV6TecLnycOHFCYWFhji4DgKTjx4+refPmji7DLvQdgHOwp99wuvDRoEEDSReL9/f3d3A1QN2Ul5ensLAwa3usDeg7AMeqTL/hdOGjbLjU39+fDgRwMHsOX2zdulUvvPCCUlNTdfLkSa1atUqxsbGSpOLiYk2ZMkWffPKJfvjhBwUEBCg6OlqzZs1S06ZNres4c+aMxo0bpzVr1sjNzU1xcXFasGCB/Pz8Kl0rfQfgWPb0G7XjYC4Ap3X+/Hl16dJFycnJ5ebl5+crLS1NU6dOVVpamj744AOlp6frf/7nf2yWGzZsmL777jtt2LBBa9eu1datWzV69GizPgIAk1mc7Yfl8vLyFBAQoNzcXL69AA5yte3QYrHYjHxcyq5du9S9e3f99NNPCg8P18GDB9WhQwft2rVL3bp1kyStW7dOAwYM0M8//2wzQlITNQOoHpVpg4x8ADBVbm6uLBaLAgMDJUnbt29XYGCgNXhIUnR0tNzc3LRz587LrqewsFB5eXk2DwC1g9Od8wHHKykpUXFxsaPLQA1yd3dXvXr1TH/fgoICTZo0Sffcc4/1m1FWVpaaNGlis1z9+vUVFBSkrKysy64rKSlJzz33XLXWV1paqqKiompdJ+BKPDw8quXye8IHrAzDUFZWlnJychxdCkwQGBiokJAQ0+6JUVxcrLvuukuGYWjJkiVVXt/kyZOVmJhofV52pv3VKioqUmZmpkpLS6tcG+Cq3NzcFBERIQ8Pjyqth/ABq7Lg0aRJE/n4+HCjJhdlGIby8/N16tQpSVJoaGiNv2dZ8Pjpp5/0xRdf2BwPDgkJsdZS5sKFCzpz5oxCQkIuu05PT095enpWS32GYejkyZOqV6+ewsLCas2N1QAzld3I7+TJkwoPD6/S/xGED0i6eKilLHg0atTI0eWghnl7e0uSTp06pSZNmtToIZiy4JGRkaFNmzaV27+ioqKUk5Oj1NRURUZGSpK++OILlZaWqkePHjVW1+9duHBB+fn5atq0qXx8fEx5T6A2aty4sU6cOKELFy7I3d39qtdD+IAkWc/xoOOtO8r+1sXFxVUKH+fOndORI0eszzMzM7Vnzx4FBQUpNDRU/+///T+lpaVp7dq1KikpsZ7HERQUJA8PD7Vv3179+vXTww8/rKVLl6q4uFgJCQkaOnSo3Ve6VFVJSYkkVXkoGXB1ZW2kpKSE8IHqw6GWuqO6/ta7d+9Wnz59rM/LzsMYMWKE/v73v+ujjz6SJN1www02r9u0aZN69+4tSXrjjTeUkJCg22+/3XqTsYULF1ZLfZXB/g9UrLraCOEDQJX07t1bFd0uyJ5bCQUFBWnlypXVWRYAJ8ZZVQAAwFSMfOCKWj71sanv9+Osgaa+X3XavHmz+vTpo99++816Ey3UYitNPgxzr3PccPpq92N77nBbHcx6nz+qDe3bUdumshj5gMvIysrSuHHj1KpVK3l6eiosLEyDBg3Sxo0bHV0aUCNGjhwpi8Uii8Uid3d3RURE6Mknn1RBQYGjS3OourBd/v73v5c7j0qSTp48qf79+5tfUCUx8gGX8OOPP+qWW25RYGCgXnjhBXXq1EnFxcVav3694uPjdejQIUeXWCVFRUVciYFL6tevn1JSUlRcXKzU1FSNGDFCFotFs2fPdnRpDlVXt0tF98ZxJox8wCU8+uijslgs+uabbxQXF6frrrtOHTt2VGJionbs2CFJmjt3rjp16iRfX1+FhYXp0Ucf1blz56zrWL58uQIDA7V69Wq1adNGXl5eiomJ0fHjxyVdDDhubm7avXu3zXvPnz9fLVq0sLkz5tdff63OnTvLy8tLPXv21IEDB6zzfv31V91zzz1q1qyZfHx81KlTJ7355ps26+zdu7cSEhI0YcIEXXPNNYqJibHrM/z0008aNGiQGjZsKF9fX3Xs2FGffPJJNW1lOCNPT0+FhIQoLCxMsbGxio6O1oYNG6zzS0tLlZSUpIiICHl7e6tLly567733bNbxySef6LrrrpO3t7f69OmjH3/88Yrvm5GRoVtvvVVeXl7q0KGDzXuWOX78uO666y4FBgYqKChIgwcPtln3rl27dMcdd+iaa65RQECAbrvtNqWlpVX6fRy5Xb766iv96U9/kre3t8LCwjR+/HidP3/eOr9ly5aaPn26hg8fLj8/P7Vo0UIfffSRTp8+rcGDB8vPz0+dO3e26Veu1BctX75czz33nPbu3Wsd4Vm+fLmki4ddVq9ebV3X/v371bdvX3l7e6tRo0YaPXq0TZ8xcuRIxcbG6h//+IdCQ0PVqFEjxcfH1/hPbLj8yIfZ5yvYqzaf1+Bszpw5o3Xr1mnGjBny9fUtN7/s2Kybm5sWLlyoiIgI/fDDD3r00Uf15JNP6qWXXrIum5+frxkzZmjFihXy8PDQo48+qqFDh+rrr79Wy5YtFR0drZSUFJsfQUtJSdHIkSNt7or5xBNPaMGCBQoJCdHTTz+tQYMG6fDhw3J3d1dBQYEiIyM1adIk+fv76+OPP9b999+va6+9Vt27d7eu47XXXtPYsWP19ddfW6dd6TPEx8erqKhIW7dula+vr77//nv5+flV27auU37dfeVlzH7PRt0qnH3gwAFt27ZNLVq0sE5LSkrSv//9by1dulRt2rTR1q1bdd9996lx48a67bbbdPz4cQ0ZMkTx8fEaPXq0du/erb/97W8Vvk9paamGDBmi4OBg7dy5U7m5uZowYYLNMsXFxYqJiVFUVJS+/PJL1a9fX9OnT1e/fv20b98+eXh46OzZsxoxYoQWLVokwzD04osvasCAAcrIyFCDBg3seh971NR2OXr0qPr166fp06fr1Vdf1enTp5WQkKCEhASlpKRYl5s3b55mzpypqVOnat68ebr//vt1880368EHH9QLL7ygSZMmafjw4fruu++sl7JW1BfdfffdOnDggNatW6fPP/9ckhQQEFDuc58/f976N9i1a5dOnTqlhx56SAkJCdawIl287D00NFSbNm3SkSNHdPfdd+uGG27Qww8/XOltbS+XDx9wfUeOHJFhGGrXrl2Fy/2+0yr7NjJmzBib8FFcXKzFixdb76z52muvqX379vrmm2/UvXt3PfTQQxozZozmzp0rT09PpaWlaf/+/frwww9t3uvZZ5/VHXfcYV1H8+bNtWrVKt11111q1qyZHn/8ceuy48aN0/r16/XOO+/YhI82bdpozpw5lfoMx44dU1xcnDp16iRJatWq1ZU2H2q5tWvXys/PTxcuXFBhYaHc3Ny0ePFiSRd/+XfmzJn6/PPPFRUVJeniPvHVV1/p5Zdf1m233aYlS5bo2muv1YsvvihJatu2rfbv31/h4YnPP/9chw4d0vr16603gps5c6bNuQZvv/22SktL9a9//cv6H2pKSooCAwO1efNm/fnPf1bfvn1t1vvPf/5TgYGB2rJli+6880673seR2yUpKUnDhg2ztss2bdpo4cKF1td7eXlJkgYMGKBHHnlEkvTMM89oyZIluummm/TXv/5VkjRp0iRFRUUpOzvbetjkSn2Rn5+f6tevX+FhlpUrV6qgoEArVqywfjFbvHixBg0apNmzZys4OFiS1LBhQy1evFj16tVTu3btNHDgQG3cuJHwAVTEnvtISBc7zKSkJB06dEh5eXm6cOGCCgoKlJ+fb73bZ/369XXTTTdZX9OuXTsFBgbq4MGD6t69u2JjYxUfH69Vq1Zp6NChWr58ufr06aOWLVvavFdZhyZdvIdF27ZtdfDgQUkX7ww4c+ZMvfPOO/rll19UVFSkwsLCcneXLbvVeGU+w/jx4zV27Fh99tlnio6OVlxcnDp37mzX9kHt1KdPHy1ZskTnz5/XvHnzVL9+fcXFxUm6GMzz8/OtQbhMUVGRbrzxRknSwYMHy93G/vf776UcPHhQYWFhNneg/eNr9u7dqyNHjqhBgwY20wsKCnT06FFJUnZ2tqZMmaLNmzfr1KlTKikpUX5+vo4dO2b3+1yOGdtl79692rdvn9544w3rNMMwVFpaqszMTLVv316SbNpg2X/4ZV8Qfj/t1KlT1jBxpb7IHgcPHlSXLl1sRoRvueUWlZaWKj093fq+HTt2tLnLcWhoqPbv32/Xe1wtwgdqvTZt2shisVR4UumPP/6oO++8U2PHjtWMGTMUFBSkr776SqNGjVJRUZHdt5X38PDQ8OHDlZKSoiFDhmjlypVasGBBpep94YUXtGDBAs2fP996/saECRPK/ZT7Hw8h2fMZHnroIcXExOjjjz/WZ599pqSkJL344osaN25cpWpE7eHr66vWrVtLkl599VV16dJFy5Yt06hRo6zH9j/++GM1a9bM5nXV9aN8l3Pu3DlFRkba/MdcpnHjxpIu3gX3119/1YIFC9SiRQt5enoqKiqqXFu4GmZsl3PnzumRRx7R+PHjy80LDw+3/vv3tyEvGwW61DRH/aLyH2+TbrFYarwWTjhFrRcUFKSYmBglJyfbnOhVpuxHy0pLS/Xiiy+qZ8+euu6663TixIlyy164cMHmxK/09HTl5ORYv8FI0kMPPaTPP/9cL730ki5cuKAhQ4aUW0/ZSa6S9Ntvv+nw4cPWdXz99dcaPHiw7rvvPnXp0kWtWrXS4cOHr/g57f0MYWFhGjNmjD744AP97W9/0yuvvHLFdcM1uLm56emnn9aUKVP03//+Vx06dJCnp6eOHTum1q1b2zzCwsIkyTqU/3u/338vpX379jp+/LhOnjx52dd07dpVGRkZatKkSbn3Ljs/4euvv9b48eM1YMAAdezYUZ6envrPf/5Tqfdx5Hbp2rWrvv/++3LraN26dZWvTrtSX+Th4WH9TaLLad++vfbu3WvTL3799ddyc3NT27Ztq1RfVRE+4BKSk5NVUlKi7t276/3331dGRoYOHjyohQsXKioqSq1bt1ZxcbEWLVqkH374Qa+//rqWLl1abj3u7u4aN26cdu7cqdTUVI0cOVI9e/a0GeZs3769evbsqUmTJumee+6x/kLs702bNk0bN27UgQMHNHLkSF1zzTXWm/60adNGGzZs0LZt23Tw4EE98sgjys7OvuJntOczTJgwQevXr1dmZqbS0tK0adMmm+AE1/fXv/5V9erVU3Jysho0aKDHH39cEydO1GuvvaajR48qLS1NixYt0muvvSZJGjNmjDIyMvTEE08oPT1dK1eutDkZ8VKio6N13XXXacSIEdq7d6++/PJL/e///q/NMsOGDdM111yjwYMH68svv1RmZqY2b96s8ePH6+eff5Z0sS28/vrrOnjwoHbu3Klhw4bZtCd73seR22XSpEnatm2bEhIStGfPHmVkZOjDDz9UQkLCVdX4e1fqi1q2bGn9Ecf//Oc/KiwsLLeOYcOGycvLSyNGjNCBAwe0adMmjRs3Tvfff7/1kIujcNgFV1Qbrsxp1aqV0tLSNGPGDP3tb3/TyZMn1bhxY0VGRmrJkiXq0qWL5s6dq9mzZ2vy5Mm69dZblZSUpOHDh9usx8fHR5MmTdK9996rX375RX/605+0bNmycu83atQobdu2TQ8++OAl65k1a5Yee+wxZWRk6IYbbtCaNWus34SmTJmiH374QTExMfLx8dHo0aMVGxur3NzcCj+jPZ+hpKRE8fHx+vnnn+Xv769+/fpp3rx5ld2cKBOzy9EVVFr9+vWVkJCgOXPmaOzYsXr++efVuHFjJSUl6YcfflBgYKC6du2qp59+WtLFwwPvv/++Jk6cqEWLFql79+6aOXPmZfdt6eJIwqpVqzRq1Ch1795dLVu21MKFC9WvXz/rMj4+Ptq6dasmTZqkIUOG6OzZs2rWrJluv/12+fv7S5KWLVum0aNHq2vXrgoLC9PMmTNtTsa2530cuV06d+6sLVu26H//93/1pz/9SYZh6Nprr9Xdd99d6fr+6Ep9UVxcnD744AP16dNHOTk51qvu/riO9evX67HHHtNNN90kHx8fxcXFae7cuVWur6oshr1n65kkLy9PAQEBys3Nte6gVcGltvYpKChQZmamIiIirGdo1zXLly/XhAkTlJOTc8Vln3/+eb377rvat29fzRdWQyr6m1d3OzRDVWq+5LZwxKW2V3KFS23hGirTF5mtuvoNDrsAlXDu3DkdOHBAixcv5iROALhKhA+gEhISEhQZGanevXtXOCwNALg8wgfw/xs5cuQVhzmXL1+uwsJCvf322zbXxQNAdbGnL6rtCB+w4WSnAKEG8bcuj20CVKy62gjhA5L+7yYz+fn5Dq4EZin7W//xBkN1UdkoVnXc3ApwZWVtpKojv1xqC0kXd6TAwECdOnVK0sVLtMruugfXYhiG8vPzderUKQUGBnL4SBcvw/Tx8dHp06fl7u5+8UcCnTGHFBQ4ugLUYaWlpTp9+rR8fHxUv37V4gPhA1ZlvylQFkDg2gIDAyv8Uaq6xGKxKDQ0VJmZmfrpp58uTjz/n4pf5Ag5mY6uAHWcm5ubwsPDq/zllPABq7IOuEmTJiouLnZ0OahB7u7ujHj8gYeHh9q0afN/h17WXvmXU0135+V/vwgwg4eHx8WRwSoifKCcevXq8R8T6iQ3N7f/u3FS0U+OLeZS6ugNAOF6OOEUAACYivABAABMRfgAAACmInwAAABTccIpAKBqVjrhPYHu5W61zoyRDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGCqSoWPv//977JYLDaPdu3aWecXFBQoPj5ejRo1kp+fn+Li4pSdnV3tRQMAgNqr0iMfHTt21MmTJ62Pr776yjpv4sSJWrNmjd59911t2bJFJ06c0JAhQ6q1YAAAULvVr/QL6tdXSEhIuem5ublatmyZVq5cqb59+0qSUlJS1L59e+3YsUM9e/aserUAAKDWq/TIR0ZGhpo2bapWrVpp2LBhOnbsmCQpNTVVxcXFio6Oti7brl07hYeHa/v27ZddX2FhofLy8mweAADAdVUqfPTo0UPLly/XunXrtGTJEmVmZupPf/qTzp49q6ysLHl4eCgwMNDmNcHBwcrKyrrsOpOSkhQQEGB9hIWFXdUHAQAAtUOlDrv079/f+u/OnTurR48eatGihd555x15e3tfVQGTJ09WYmKi9XleXh4BBAAAF1alS20DAwN13XXX6ciRIwoJCVFRUZFycnJslsnOzr7kOSJlPD095e/vb/MAUHts3bpVgwYNUtOmTWWxWLR69Wqb+YZh6JlnnlFoaKi8vb0VHR2tjIwMm2XOnDmjYcOGyd/fX4GBgRo1apTOnTtn4qcAYKYqhY9z587p6NGjCg0NVWRkpNzd3bVx40br/PT0dB07dkxRUVFVLhSAczp//ry6dOmi5OTkS86fM2eOFi5cqKVLl2rnzp3y9fVVTEyMCgoKrMsMGzZM3333nTZs2KC1a9dq69atGj16tFkfAYDJKnXY5fHHH9egQYPUokULnThxQs8++6zq1aune+65RwEBARo1apQSExMVFBQkf39/jRs3TlFRUVzpAriw/v372xyS/T3DMDR//nxNmTJFgwcPliStWLFCwcHBWr16tYYOHaqDBw9q3bp12rVrl7p16yZJWrRokQYMGKB//OMfatq06SXXXVhYqMLCQutzTlYHao9KjXz8/PPPuueee9S2bVvdddddatSokXbs2KHGjRtLkubNm6c777xTcXFxuvXWWxUSEqIPPvigRgoH4PwyMzOVlZVlcxVcQECAevToYb0Kbvv27QoMDLQGD0mKjo6Wm5ubdu7cedl1c7I6UHtVauTjrbfeqnC+l5eXkpOTLzv8CufW8qmPHV3CJf04a6CjS8BVKrvSLTg42Gb676+Cy8rKUpMmTWzm169fX0FBQRVeKcfJ6kDtVembjAGAM/D09JSnp6ejywBwFfhhOQA1puxKtz/+xtPvr4ILCQnRqVOnbOZfuHBBZ86cqfBKOQC1F+EDQI2JiIhQSEiIzVVweXl52rlzp/UquKioKOXk5Cg1NdW6zBdffKHS0lL16NHD9JoB1DwOuwCoknPnzunIkSPW55mZmdqzZ4+CgoIUHh6uCRMmaPr06WrTpo0iIiI0depUNW3aVLGxsZKk9u3bq1+/fnr44Ye1dOlSFRcXKyEhQUOHDr3slS4AajfCB4Aq2b17t/r06WN9XnYS6IgRI7R8+XI9+eSTOn/+vEaPHq2cnBz16tVL69atk5eXl/U1b7zxhhISEnT77bfLzc1NcXFxWrhwoemfBYA5CB8AqqR3794yDOOy8y0Wi6ZNm6Zp06ZddpmgoCCtXLmyJsoD4IQ45wMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpqhQ+Zs2aJYvFogkTJlinFRQUKD4+Xo0aNZKfn5/i4uKUnZ1d1ToBAICLuOrwsWvXLr388svq3LmzzfSJEydqzZo1evfdd7VlyxadOHFCQ4YMqXKhAADANVxV+Dh37pyGDRumV155RQ0bNrROz83N1bJlyzR37lz17dtXkZGRSklJ0bZt27Rjx45qKxoAANReVxU+4uPjNXDgQEVHR9tMT01NVXFxsc30du3aKTw8XNu3b7/kugoLC5WXl2fzAAAArqvS4eOtt95SWlqakpKSys3LysqSh4eHAgMDbaYHBwcrKyvrkutLSkpSQECA9REWFlbZkgA4uZKSEk2dOlURERHy9vbWtddeq+eff16GYViXMQxDzzzzjEJDQ+Xt7a3o6GhlZGQ4sGoANaVS4eP48eN67LHH9MYbb8jLy6taCpg8ebJyc3Otj+PHj1fLegE4j9mzZ2vJkiVavHixDh48qNmzZ2vOnDlatGiRdZk5c+Zo4cKFWrp0qXbu3ClfX1/FxMSooKDAgZUDqAn1K7NwamqqTp06pa5du1qnlZSUaOvWrVq8eLHWr1+voqIi5eTk2Ix+ZGdnKyQk5JLr9PT0lKen59VVD6BW2LZtmwYPHqyBAwdKklq2bKk333xT33zzjaSLox7z58/XlClTNHjwYEnSihUrFBwcrNWrV2vo0KEOqx1A9avUyMftt9+u/fv3a8+ePdZHt27dNGzYMOu/3d3dtXHjRutr0tPTdezYMUVFRVV78QBqh5tvvlkbN27U4cOHJUl79+7VV199pf79+0uSMjMzlZWVZXO+WEBAgHr06MH5YoALqtTIR4MGDXT99dfbTPP19VWjRo2s00eNGqXExEQFBQXJ399f48aNU1RUlHr27Fl9VQOoVZ566inl5eWpXbt2qlevnkpKSjRjxgwNGzZMkqznhAUHB9u87krniz333HM1WziAGlGp8GGPefPmyc3NTXFxcSosLFRMTIxeeuml6n4bALXIO++8ozfeeEMrV65Ux44dtWfPHk2YMEFNmzbViBEjrmqdkydPVmJiovV5Xl4eJ6wDtUSVw8fmzZttnnt5eSk5OVnJyclVXTUAF/HEE0/oqaeesp670alTJ/30009KSkrSiBEjrOeEZWdnKzQ01Pq67Oxs3XDDDZdcJ+eLAbUXv+0CoMbl5+fLzc22u6lXr55KS0slSREREQoJCbE5XywvL087d+7kfDHABVX7YRcA+KNBgwZpxowZCg8PV8eOHfXtt99q7ty5evDBByXJ+htR06dPV5s2bRQREaGpU6eqadOmio2NdWzxAKod4QNAjVu0aJGmTp2qRx99VKdOnVLTpk31yCOP6JlnnrEu8+STT+r8+fMaPXq0cnJy1KtXL61bt67a7ikEwHkQPgDUuAYNGmj+/PmaP3/+ZZexWCyaNm2apk2bZl5hAByCcz4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVPUdXQAAAHXGSoujK7i0ew1T346RDwAAYCrCBwAAMBXhAwAAmIrwAQAATFWp8LFkyRJ17txZ/v7+8vf3V1RUlD799FPr/IKCAsXHx6tRo0by8/NTXFycsrOzq71oAABQe1UqfDRv3lyzZs1Samqqdu/erb59+2rw4MH67rvvJEkTJ07UmjVr9O6772rLli06ceKEhgwZUiOFAwCA2qlSl9oOGjTI5vmMGTO0ZMkS7dixQ82bN9eyZcu0cuVK9e3bV5KUkpKi9u3ba8eOHerZs2f1VQ0AAGqtqz7no6SkRG+99ZbOnz+vqKgopaamqri4WNHR0dZl2rVrp/DwcG3fvv2y6yksLFReXp7NA4Dr+eWXX3TfffepUaNG8vb2VqdOnbR7927rfMMw9Mwzzyg0NFTe3t6Kjo5WRkaGAysGUFMqHT72798vPz8/eXp6asyYMVq1apU6dOigrKwseXh4KDAw0Gb54OBgZWVlXXZ9SUlJCggIsD7CwsIq/SEAOLfffvtNt9xyi9zd3fXpp5/q+++/14svvqiGDRtal5kzZ44WLlyopUuXaufOnfL19VVMTIwKCgocWDmAmlDpO5y2bdtWe/bsUW5urt577z2NGDFCW7ZsueoCJk+erMTEROvzvLw8AgjgYmbPnq2wsDClpKRYp0VERFj/bRiG5s+frylTpmjw4MGSpBUrVig4OFirV6/W0KFDTa8ZQM2p9MiHh4eHWrdurcjISCUlJalLly5asGCBQkJCVFRUpJycHJvls7OzFRISctn1eXp6Wq+eKXsAcC0fffSRunXrpr/+9a9q0qSJbrzxRr3yyivW+ZmZmcrKyrI5bBsQEKAePXpc9rAth2yB2qvK9/koLS1VYWGhIiMj5e7uro0bN1rnpaen69ixY4qKiqrq2wCoxX744QctWbJEbdq00fr16zV27FiNHz9er732miRZD80GBwfbvK6iw7YcsgVqr0oddpk8ebL69++v8PBwnT17VitXrtTmzZu1fv16BQQEaNSoUUpMTFRQUJD8/f01btw4RUVFcaULUMeVlpaqW7dumjlzpiTpxhtv1IEDB7R06VKNGDHiqtbJIVug9qpU+Dh16pSGDx+ukydPKiAgQJ07d9b69et1xx13SJLmzZsnNzc3xcXFqbCwUDExMXrppZdqpHAAtUdoaKg6dOhgM619+/Z6//33Jcl6aDY7O1uhoaHWZbKzs3XDDTdccp2enp7y9PSsmYIB1KhKhY9ly5ZVON/Ly0vJyclKTk6uUlEAXMstt9yi9PR0m2mHDx9WixYtJF08+TQkJEQbN260ho28vDzt3LlTY8eONbtcADWs0le7AEBlTZw4UTfffLNmzpypu+66S998843++c9/6p///KckyWKxaMKECZo+fbratGmjiIgITZ06VU2bNlVsbKxjiwdQ7QgfAGrcTTfdpFWrVmny5MmaNm2aIiIiNH/+fA0bNsy6zJNPPqnz589r9OjRysnJUa9evbRu3Tp5eXk5sHIANYHwAcAUd955p+68887LzrdYLJo2bZqmTZtmYlUAHKHKl9oCAABUBuEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgqkqFj6SkJN10001q0KCBmjRpotjYWKWnp9ssU1BQoPj4eDVq1Eh+fn6Ki4tTdnZ2tRYNAABqr0qFjy1btig+Pl47duzQhg0bVFxcrD//+c86f/68dZmJEydqzZo1evfdd7VlyxadOHFCQ4YMqfbCAQBA7VSp8LFu3TqNHDlSHTt2VJcuXbR8+XIdO3ZMqampkqTc3FwtW7ZMc+fOVd++fRUZGamUlBRt27ZNO3bsuOQ6CwsLlZeXZ/MA4LpmzZoli8WiCRMmWKcxYgrULVU65yM3N1eSFBQUJElKTU1VcXGxoqOjrcu0a9dO4eHh2r59+yXXkZSUpICAAOsjLCysKiUBcGK7du3Syy+/rM6dO9tMZ8QUqFuuOnyUlpZqwoQJuuWWW3T99ddLkrKysuTh4aHAwECbZYODg5WVlXXJ9UyePFm5ubnWx/Hjx6+2JABO7Ny5cxo2bJheeeUVNWzY0Dr9akZMJUZNgdrsqsNHfHy8Dhw4oLfeeqtKBXh6esrf39/mAcD1xMfHa+DAgTYjo9LVjZhKjJoCtdlVhY+EhAStXbtWmzZtUvPmza3TQ0JCVFRUpJycHJvls7OzFRISUqVCAdReb731ltLS0pSUlFRu3tWMmEqMmgK1WaXCh2EYSkhI0KpVq/TFF18oIiLCZn5kZKTc3d21ceNG67T09HQdO3ZMUVFR1VMxgFrl+PHjeuyxx/TGG2/Iy8ur2tbLqClQe9WvzMLx8fFauXKlPvzwQzVo0MD6rSQgIEDe3t4KCAjQqFGjlJiYqKCgIPn7+2vcuHGKiopSz549a+QDAHBuqampOnXqlLp27WqdVlJSoq1bt2rx4sVav369dcT096MfjJgCrqtS4WPJkiWSpN69e9tMT0lJ0ciRIyVJ8+bNk5ubm+Li4lRYWKiYmBi99NJL1VIsgNrn9ttv1/79+22mPfDAA2rXrp0mTZqksLAw64hpXFycJEZMAVdXqfBhGMYVl/Hy8lJycrKSk5OvuigArqNBgwbWK+LK+Pr6qlGjRtbpjJgCdUulwgcA1ARGTIG6hfABwHSbN2+2ec6IKVC38Ku2AADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFPVd3QBQG3U8qmPHV3CJf04a6CjSwCAK2LkAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwVaXDx9atWzVo0CA1bdpUFotFq1evtplvGIaeeeYZhYaGytvbW9HR0crIyKiuegEAQC1X6fBx/vx5denSRcnJyZecP2fOHC1cuFBLly7Vzp075evrq5iYGBUUFFS5WAC1U1JSkm666SY1aNBATZo0UWxsrNLT022WKSgoUHx8vBo1aiQ/Pz/FxcUpOzvbQRUDqEmVDh/9+/fX9OnT9Ze//KXcPMMwNH/+fE2ZMkWDBw9W586dtWLFCp04caLcCAmAumPLli2Kj4/Xjh07tGHDBhUXF+vPf/6zzp8/b11m4sSJWrNmjd59911t2bJFJ06c0JAhQxxYNYCaUr86V5aZmamsrCxFR0dbpwUEBKhHjx7avn27hg4dWu41hYWFKiwstD7Py8urzpIAOIF169bZPF++fLmaNGmi1NRU3XrrrcrNzdWyZcu0cuVK9e3bV5KUkpKi9u3ba8eOHerZs6cjygZQQ6r1hNOsrCxJUnBwsM304OBg67w/SkpKUkBAgPURFhZWnSUBcEK5ubmSpKCgIElSamqqiouLbb64tGvXTuHh4dq+ffsl11FYWKi8vDybB4DaweFXu0yePFm5ubnWx/Hjxx1dEoAaVFpaqgkTJuiWW27R9ddfL+niFxcPDw8FBgbaLMsXF8A1VWv4CAkJkaRyJ4llZ2db5/2Rp6en/P39bR4AXFd8fLwOHDigt956q0rr4YsLUHtVa/iIiIhQSEiINm7caJ2Wl5ennTt3KioqqjrfCkAtlJCQoLVr12rTpk1q3ry5dXpISIiKioqUk5NjszxfXADXVOnwce7cOe3Zs0d79uyRdPEk0z179ujYsWOyWCyaMGGCpk+fro8++kj79+/X8OHD1bRpU8XGxlZz6QBqC8MwlJCQoFWrVumLL75QRESEzfzIyEi5u7vbfHFJT0/XsWPH+OICuKBKX+2ye/du9enTx/o8MTFRkjRixAgtX75cTz75pM6fP6/Ro0crJydHvXr10rp16+Tl5VV9VQOoVeLj47Vy5Up9+OGHatCggfU8joCAAHl7eysgIECjRo1SYmKigoKC5O/vr3HjxikqKoorXQAXVOnw0bt3bxmGcdn5FotF06ZN07Rp06pUGADXsWTJEkkX+4/fS0lJ0ciRIyVJ8+bNk5ubm+Li4lRYWKiYmBi99NJLJlcKwAzVep8PALiUir6wlPHy8lJycvJl754MwHU4/FJbAABQtxA+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqQgfAADAVIQPAABgKsIHAAAwFeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhA8AAGAqwgcAADAV4QMAAJiK8AEAAExF+AAAAKYifAAAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATEX4AAAApiJ8AAAAUxE+AACAqWosfCQnJ6tly5by8vJSjx499M0339TUWwFwEfQbQN1QI+Hj7bffVmJiop599lmlpaWpS5cuiomJ0alTp2ri7QC4APoNoO6okfAxd+5cPfzww3rggQfUoUMHLV26VD4+Pnr11Vdr4u0AuAD6DaDuqF/dKywqKlJqaqomT55snebm5qbo6Ght37693PKFhYUqLCy0Ps/NzZUk5eXlVUs9pYX51bKe6lZdn686sa3s5+rbqmw9hmFUy/qupLL9hlTzfYec8U/shG1BEtuqMpxxW0nVsr0q029Ue/j4z3/+o5KSEgUHB9tMDw4O1qFDh8otn5SUpOeee67c9LCwsOouzakEzHd0BbUH28p+1b2tzp49q4CAgOpd6SVUtt+Q6mjf8XDN/y1cBtuqcqpxe9nTb1R7+KisyZMnKzEx0fq8tLRUZ86cUaNGjWSxWBxYma28vDyFhYXp+PHj8vf3d3Q5To1tZT9n3VaGYejs2bNq2rSpo0u5rNrQdzjr39dZsb3s54zbqjL9RrWHj2uuuUb16tVTdna2zfTs7GyFhISUW97T01Oenp420wIDA6u7rGrj7+/vNH9oZ8e2sp8zbiszRjzKVLbfkGpX3+GMf19nxvayn7NtK3v7jWo/4dTDw0ORkZHauHGjdVppaak2btyoqKio6n47AC6AfgOoW2rksEtiYqJGjBihbt26qXv37po/f77Onz+vBx54oCbeDoALoN8A6o4aCR933323Tp8+rWeeeUZZWVm64YYbtG7dunInk9Umnp6eevbZZ8sN86I8tpX92Fb/h34DbC/71fZtZTHMupYOAABA/LYLAAAwGeEDAACYivABAABMRfgAAACmInwAAABTET4AAICpCB8AAMBUhI8r+PLLL3XfffcpKipKv/zyiyTp9ddf11dffeXgyhwvLy/P7gfKY99yLbSHq0dbsJ+rbCvCRwXef/99xcTEyNvbW99++60KCwslSbm5uZo5c6aDq3O8wMBANWzYsMJH2TKwxb7lemgPV4e2YD9X2lbc4bQCN954oyZOnKjhw4erQYMG2rt3r1q1aqVvv/1W/fv3V1ZWlqNLdKgtW7bYvextt91Wg5XUPuxbrof2cHVoC/ZzpW1VI7/t4irS09N16623lpseEBCgnJwc8wtyMnSgV499y/XQHq4ObcF+rrStCB8VCAkJ0ZEjR9SyZUub6V999ZVatWrlmKKcXH5+vo4dO6aioiKb6Z07d3ZQRc6JfatuoD1cGW3Bfq60rQgfFXj44Yf12GOP6dVXX5XFYtGJEye0fft2Pf7445o6daqjy3Mqp0+f1gMPPKBPP/30kvNLSkpMrsi5sW+5NtqD/WgL9nOpbWXgskpLS43p06cbvr6+hsViMSwWi+Hl5WVMmTLF0aU5nXvvvde45ZZbjF27dhm+vr7GZ599Zrz++utG27ZtjbVr1zq6PKfDvuXaaA/2oy3Yz5W2FSec2qGoqEhHjhzRuXPn1KFDB/n5+Tm6JKcTGhqqDz/8UN27d5e/v792796t6667Th999JHmzJlT6y4DMwv7lmuiPVQebcF+rrCtuNS2Av/+97+Vn58vDw8PdejQQd27d6+Vf2QznD9/Xk2aNJEkNWzYUKdPn5YkderUSWlpaY4szSmxb7k22oP9aAv2c6VtRfiowMSJE9WkSRPde++9+uSTTzhOW4G2bdsqPT1dktSlSxe9/PLL+uWXX7R06VKFhoY6uDrnw77l2mgP9qMt2M+VthXhowInT57UW2+9JYvForvuukuhoaGKj4/Xtm3bHF2a03nsscd08uRJSdKzzz6rTz/9VOHh4Vq4cGGtu/mNGdi3XBvtwX60Bfu50rbinA875efna9WqVVq5cqU+//xzNW/eXEePHnV0WU4rPz9fhw4dUnh4uK655hpHl+PU2LdcH+3BPrQF+9X2bcWltnby8fFRTEyMfvvtN/300086ePCgo0tyGsXFxWrXrp3Wrl2r9u3bS7q4vbp27ergymoH9i3XQnu4erQF+9X2bcVhlyvIz8/XG2+8oQEDBqhZs2aaP3++/vKXv+i7775zdGlOw93dXQUFBY4uo9Zh33JNtIfKoy3Yz1W2FYddKjB06FCtXbtWPj4+uuuuuzRs2DBFRUU5uiynNHPmTB0+fFj/+te/VL8+A2pXwr7l2mgP9qMt2M+VthWtogL16tXTO++8o5iYGNWrV8/R5Ti1Xbt2aePGjfrss8/UqVMn+fr62sz/4IMPHFSZc2Lfcm20B/vRFuznStuKkQ9UiwceeKDC+SkpKSZVAjge7QGoGOHjDxYuXKjRo0fLy8tLCxcurHDZ8ePHm1QVXAH7FnARbcF+rrqtCB9/EBERod27d6tRo0aKiIi47HIWi0U//PCDiZU5t759++qDDz5QYGCgzfS8vDzFxsbqiy++cExhToR9q+6gPVSMtmA/V91WhA9UCzc3N2VlZVlvKV3m1KlTatasmYqLix1UGWA+2gNQMS61rcC0adOUn59fbvp///tfTZs2zQEVOZ99+/Zp3759kqTvv//e+nzfvn369ttvtWzZMjVr1szBVTof9i3XRHuoPNqC/VxpWzHyUYF69erp5MmT5b69/Prrr2rSpEmtvq9+dXFzc5PFYpEkXWpX8vb21qJFi/Tggw+aXZpTY99yTbSHyqMt2M+VthWX2lbAMAxrR/J7e/fuVVBQkAMqcj6ZmZkyDEOtWrXSN998o8aNG1vneXh4qEmTJrX+krCawL7lmmgPlUdbsJ8rbSvCxyU0bNhQFotFFotF1113nc0fu6SkROfOndOYMWMcWKHzaNGihSSptLTUwZXUDuxbro32YD/agv1ccVtx2OUSXnvtNRmGoQcffFDz589XQECAdZ6Hh4datmxZa+8qV1NWrFhR4fzhw4ebVIlzY9+qG2gPV0ZbsJ8rbivCRwW2bNmim2++We7u7o4uxek1bNjQ5nlxcbHy8/Pl4eEhHx8fnTlzxkGVOSf2LddGe7AfbcF+rrStCB92KigoUFFRkc00f39/B1VTO2RkZGjs2LF64oknFBMT4+hyHC4vL8+6z+Tl5VW4LPuW66E9XBn9bHmu2m8QPiqQn5+vJ598Uu+8845+/fXXcvNr05nFjrJ7927dd999OnTokKNLcbjfn6n++6sifq/shDL2LddEeyiPfrZirtpvcMJpBZ544glt2rRJS5Ys0f3336/k5GT98ssvevnllzVr1ixHl1cr1K9fXydOnHB0GU7hiy++sJ6RvmnTJgdXA0egPZRHP1sxV+03GPmoQHh4uFasWKHevXvL399faWlpat26tV5//XW9+eab+uSTTxxdotP46KOPbJ4bhqGTJ09q8eLFCgsL06effuqgygDz0R7sRz9bNzHyUYEzZ86oVatWki4eSys7SaxXr14aO3asI0tzOrGxsTbPLRaLGjdurL59++rFF190TFFObN26dfLz81OvXr0kScnJyXrllVfUoUMHJScnlzthEbUL7cF+9LP2c6V+g9urV6BVq1bKzMyUJLVr107vvPOOJGnNmjXlfjCqristLbV5lJSUKCsrSytXrlRoaKijy3M6TzzxhPXksf379ysxMVEDBgxQZmamEhMTHVwdqor2YD/6Wfu5VL9h4LLmzp1rLFiwwDAMw9iwYYPh5eVleHp6Gm5ubsb8+fMdXJ1zKiwsNA4dOmQUFxc7uhSn5uvra2RmZhqGYRjPPvusERcXZxiGYaSmphrBwcEOrAzVifZwZfSz9nOlfoPDLhWYOHGi9d/R0dE6dOiQUlNT1bp1a3Xu3NmBlTmf/Px8JSQkWG+udPjwYbVq1Urjxo1Ts2bN9NRTTzm4Qufi4eFh/YGozz//3HrTqaCgoCteTgfnR3uwH/2s/Vyp3+CwSyW0aNFCQ4YMoUFcwuTJk7Vv3z5t3rxZXl5e1unR0dF6++23HViZc+rVq5cSExP1/PPP65tvvtHAgQMlXfxPqnnz5g6uDlVFe7h69LOX50r9BiMfFVi4cOElp1ssFnl5eal169a69dZb+aEoSatXr9bbb7+tnj172lyH3rFjRx09etSBlTmnxYsX69FHH9V7772nJUuWWH9m/dNPP1W/fv0cXB2qivZgP/pZ+7lSv8GlthWIiIjQ6dOnlZ+fbz2L+LfffpOPj4/8/Px06tQptWrVSps2bVJYWJiDq3UsHx8fHThwQK1atVKDBg20d+9etWrVSnv37tWtt96q3NxcR5cImIb2YD/62bqJwy4VmDlzpm666SZlZGTo119/1a+//qrDhw+rR48eWrBggY4dO6aQkBCbY5Z1Vbdu3fTxxx9bn5d92/vXv/5V637wyCwlJSV6//33NX36dE2fPl2rVq2qVXcoxOXRHuxHP1s5LtNvOPqMV2fWqlUr49tvvy03PS0tzYiIiDAMwzC+/vprIyQkxOTKnM+XX35p+Pn5GWPGjDG8vLyMxx57zLjjjjsMX19fY/fu3Y4uz+lkZGQYbdq0MXx8fIwbb7zRuPHGGw0fHx+jbdu2xpEjRxxdHqqI9mA/+ln7uVK/wchHBU6ePKkLFy6Um37hwgVlZWVJkpo2baqzZ8+aXZrT6dWrl/bs2aMLFy6oU6dO+uyzz9SkSRNt375dkZGRji7P6YwfP17XXnutjh8/rrS0NKWlpenYsWOKiIjQ+PHjHV0eqoj2YD/6Wfu5VL/h6PTjzAYMGGB07drVSEtLs05LS0szIiMjjYEDBxqGYRgfffSRcf311zuqRNRSPj4+xr59+8pN37Nnj+Hr6+uAigDHoJ+1nyv1G4x8VGDZsmUKCgpSZGSkPD095enpqW7duikoKEjLli2TJPn5+dXp2yW7ubmpXr16FT7q1+eiqj/y9PS85De5c+fOycPDwwEVoTrQHiqPftZ+rtRvcLWLHQ4dOqTDhw9Lktq2bau2bds6uCLn8eGHH1523vbt27Vw4UKVlpaqoKDAxKqc3/Dhw5WWlqZly5ape/fukqSdO3fq4YcfVmRkpJYvX+7YAnFVaA9Xj372ylyq33D00EttwC2SK+fQoUNGbGysUa9ePWP48OHGjz/+6OiSnM5vv/1mDB482HBzczM8PDwMDw8Pw83NzYiNjTVycnIcXR6qEe3BPvSzV+ZK/QaHXSqQn5+vUaNGycfHRx07dtSxY8ckSePGjdOsWbMcXJ3zOXHihB5++GF16tRJFy5c0J49e/Taa6+pRYsWji7NaZSWlmr27NkaOHCgfvnlF8XGxurdd9/Ve++9p/T0dK1atUoBAQGOLhPVgPZgH/rZK3PFfoPwUYHJkydr79693CL5CnJzczVp0iS1bt1a3333nTZu3Kg1a9bo+uuvd3RpTmfGjBl6+umn5efnp2bNmumTTz7R6tWrNWjQILVu3drR5aEa0B4qh372ylyy33D00IszCw8PN7Zv324YhmH4+fkZR48eNQzj4rXWDRo0cGRpTmP27NlGUFCQ0aFDB2P16tWOLsfptW7d2li6dKn1+YYNGwwPDw+jpKTEgVWhutAeKo9+9spcsd/ghNMKcIvkK3Nzc5O3t7eio6Mr/O2FDz74wMSqnJenp6eOHDlic5toLy8vHTlypNb9MBTKoz1UHv3slbliv8E1XxUou0XyuHHjJHGL5EsZPny4zQ9noWIXLlywGVqWJHd3dxUXFzuoIlQn2kPl0c9emSv2G4SPCsycOVP9+/fX999/rwsXLmjBggX6/vvvtW3bNm3ZssXR5TmFWnVplxMwDEMjR46Up6endVpBQYHGjBkjX19f6zS+GddOtIfKo5+9MlfsNzjscgVHjx7VrFmztHfvXp07d05du3bVpEmT1KlTJ0eXhlrogQcesGu5lJSUGq4EcB70sxVzxX6D8AEAAEzFYZdLcHNzu+JxW4vFcskfQwIAXBn9bN1G+LiEVatWXXbe72+RDAC4OvSzdRuHXeyUnp6up556SmvWrNGwYcM0bdo07lQIANWIfrbu4A6nV8AtkgGgZtHP1j2Ej8vgFskAULPoZ+suzvm4hDlz5mj27NkKCQnRm2++qcGDBzu6JABwKfSzdRvnfFwCt0gGgJpFP1u3MfJxCdwiGQBqFv1s3cbIBwAAMBUnnAIAAFMRPgAAgKkIHwAAwFSEDwAAYCrCBwAAMBXhAwAAmIrwAQAATPX/AQKfug4B0e1fAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 5**\n",
        "\n",
        "Соберите все 3 функции в одной ячейке ниже. Проанализируйте получшившийся код. Сделайте ревью!\n",
        "\n",
        "Попробуйте сделать **рефакторинг**: найти повторы в коде, избыточные конструкции и заменить их на более простые и питоничные решения. Рефакторинг ускоряет выполнение кода и упрощает его чтение.\n",
        "\n",
        "Улучшите код, добавьте строчки для вывода промежуточных и итоговых результатов; запустите ячейку.\n",
        "\n",
        "Это творческое задание, правильных решений нет - главное, чтобы код работал! :)\n",
        "\n",
        "Не забудьте закомментировать свои изменения."
      ],
      "metadata": {
        "id": "Y2SS-kAkcZcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### №1\n",
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all(tag, class_)\n",
        "    text = []\n",
        "    for p in paragraphs:\n",
        "      text.append(p.get_text(strip=True))\n",
        "    return text\n",
        "  else:\n",
        "      print(f\"{response.status_code} Error\")\n",
        "      return []\n",
        "\n",
        "### review\n",
        "### строки 8-10 можно сократить с помощью list comprehension:\n",
        "### text = [p.get_text(strip=True) for p in paragraphs]\n",
        "### добавим промежуточный вывод результатов после строки 5\n",
        "### print(f\"Статус запроса: {response.status_code} - ответ получен\"\n",
        "\n",
        "### новая версия\n",
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200:\n",
        "    print(f\"Статус запроса: {response.status_code} - ответ получен\")\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all(tag, class_)\n",
        "    text = [p.get_text(strip=True) for p in paragraphs]\n",
        "    return text\n",
        "  else:\n",
        "      print(f\"{response.status_code} Error\")\n",
        "      return []\n",
        ""
      ],
      "metadata": {
        "id": "mU0kfLfvbq0-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_text_from_url(url, tag, class_=None):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code == 200:\n",
        "    print(f\"Статус запроса: {response.status_code} - ответ получен\")\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all(tag, class_)\n",
        "    text = [p.get_text(strip=True) for p in paragraphs]\n",
        "    return text\n",
        "  else:\n",
        "      print(f\"{response.status_code} Error\")\n",
        "      return []"
      ],
      "metadata": {
        "id": "09FVY12VAjFV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### №2\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split()\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = []\n",
        "    for token in split_text:\n",
        "      if token not in stopwords:\n",
        "        clean_text.append(token)\n",
        "    words = clean_text\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)\n",
        "\n",
        "### review\n",
        "### в строках 7-11 применим list comprehension\n",
        "### clean_text = [token for token in split_text if token not in stopwords\n",
        "### выведем промежуточный результат для clean_text\n",
        "### print(f\"Чистый текст: {clean_text[:10]}...\")\n",
        "\n",
        "### новая версия\n",
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split()\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = [token for token in split_text if token not in stopwords]\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    print(f\"Чистый текст: {clean_text[:10]}\")\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)"
      ],
      "metadata": {
        "id": "G79cUlxw6jna"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text, positive_words, negative_words): ### ваш код здесь ###\n",
        "    lower_text = text.lower()\n",
        "    split_text = lower_text.split()\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "      stopwords = f.read().split()\n",
        "    clean_text = [token for token in split_text if token not in stopwords]\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    print(f\"Чистый текст: {clean_text[:10]}\")\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif positive_count < negative_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "    return(sentiment)"
      ],
      "metadata": {
        "id": "bu54Og0rAfAb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### №3\n",
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_)\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = []\n",
        "  for sentence in sentences:\n",
        "    score = analyze_sentiment(sentence, positive_words, negative_words)\n",
        "    sentiments.append(score)\n",
        "  return(sentiments)\n",
        "\n",
        "  ### review\n",
        "  ### применим list comprehension\n",
        "  ### sentiments = [analyze_sentiment(sentence, positive_words, negative_words) for sentence in sentences]\n",
        "  ### напечатаем первые 200 символов текста для примера\n",
        "  ###   print(f\"Обработанный текст: {full_text[:100]}\")\n",
        "\n",
        "  ### новая версия\n",
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_)\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = [analyze_sentiment(sentence, positive_words, negative_words) for sentence in sentences]\n",
        "  print(f\"Обработанный текст: {full_text[:100]}\")\n",
        "  return(sentiments)"
      ],
      "metadata": {
        "id": "_aeZJxfo7zov"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze(positive_words, negative_words, url, tag, class_= None):\n",
        "  scraped_text = scrape_text_from_url(url, tag, class_)\n",
        "  full_text = \" \".join(scraped_text)\n",
        "  sentences = full_text.split('.')\n",
        "  sentiments = [analyze_sentiment(sentence, positive_words, negative_words) for sentence in sentences]\n",
        "  print(f\"Обработанный текст: {full_text[:100]}\")\n",
        "  return(sentiments)"
      ],
      "metadata": {
        "id": "AYvTpxa6AcJH"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}